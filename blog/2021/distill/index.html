<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Investigation about Optimal LLM Scale for 1-Bit Training | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content="As the bit width decreases, the model size required to maintain the same level of performance increases; conversely, if the model becomes excessively large, the benefits of memory and compute reduction are significantly diminished. In this work, we quantitatively evaluate the “minimum model scale needed to achieve performance comparable to an FP16 baseline” for each of the leading 1-bit and sub-1-bit quantization methods published since 2023."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hkhkhzx.github.io/blog/2021/distill/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Investigation about Optimal LLM Scale for 1-Bit Training",
            "description": "As the bit width decreases, the model size required to maintain the same level of performance increases; conversely, if the model becomes excessively large, the benefits of memory and compute reduction are significantly diminished. In this work, we quantitatively evaluate the “minimum model scale needed to achieve performance comparable to an FP16 baseline” for each of the leading 1-bit and sub-1-bit quantization methods published since 2023."",
            "published": "May 22, 2021",
            "authors": [
              
              {
                "author": "Hyeon-seok Shin",
                "authorURL": "https://en.wikipedia.org/wiki/Albert_Einstein",
                "affiliations": [
                  {
                    "name": "POSTECH, Republic of Korea",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Investigation about Optimal LLM Scale for 1-Bit Training</h1> <p>As the bit width decreases, the model size required to maintain the same level of performance increases; conversely, if the model becomes excessively large, the benefits of memory and compute reduction are significantly diminished. In this work, we quantitatively evaluate the “minimum model scale needed to achieve performance comparable to an FP16 baseline” for each of the leading 1-bit and sub-1-bit quantization methods published since 2023."</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#related-work-about-quantization-method-1-bit-or-sub-1-bit">Related work about quantization method (1-bit or sub 1-bit)</a> </div> <ul> <li> <a href="#bitnet">BitNet</a> </li> <li> <a href="#bitnet-1-58">BitNet 1.58</a> </li> <li> <a href="#sub-1-bit-quantization-stbllm">Sub 1-bit Quantization, STBLLM</a> </li> </ul> <div> <a href="#methodology-for-finding-the-optimal-model-size-results">Methodology for Finding the Optimal Model Size &amp; Results</a> </div> <ul> <li> <a href="#zero-shot-accuracy-perspective">Zero-shot Accuracy Perspective</a> </li> <li> <a href="#perplexity-perspective">Perplexity Perspective</a> </li> <li> <a href="#holistic-evaluation">Holistic Evaluation</a> </li> </ul> <div> <a href="#conclusion">Conclusion</a> </div> <div> <a href="#mermaid">Mermaid</a> </div> <div> <a href="#diff2html">Diff2Html</a> </div> <div> <a href="#leaflet">Leaflet</a> </div> <div> <a href="#chartjs-echarts-and-vega-lite">Chartjs, Echarts and Vega-Lite</a> </div> <div> <a href="#tikz">TikZ</a> </div> <div> <a href="#typograms">Typograms</a> </div> <div> <a href="#layouts">Layouts</a> </div> <div> <a href="#other-typography">Other Typography?</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Since the introduction of the Transformer architecture, large language models have leveraged tens to hundreds of billions of parameters to achieve remarkable language generation capabilities. However, such massive model scales dramatically increase cloud operating costs, GPU memory requirements, and power consumption, making real-world deployment and low-power edge use difficult. To mitigate these challenges, 8-bit and 4-bit quantization techniques have been adopted to reduce memory usage with minimal performance loss compared to FP16, but when bit-width is reduced below 2 bits, both perplexity and zero-shot accuracy suffer drastic degradation.</p> <div style="text-align: center;"> <img src="/assets/img/modelsizeup.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br> Extreme 1-bit quantization offers the potential to reduce storage and bandwidth requirements by up to 32× [2], but to offset the information loss from lower bit-widths, the model must be scaled up significantly. Conversely, an overly large model diminishes the gains in memory and compute reduction, so identifying the “optimal model size” that maintains FP16-level performance while still achieving efficient resource savings is essential.</p> <p>In particular, PTQ (Post-Training Quantization) and QAT (Quantization-Aware Training) require very different model scales, yet to date no study has quantitatively defined the optimal sizes for these two approaches. In this work, we conduct a thorough survey of the leading 1-bit training methods published since 2023 and experimentally determine the PTQ- and QAT-optimized model scales, providing concrete guidelines for balancing target performance and resource constraints when designing and deploying 1-bit LLMs.</p> <h2 id="related-work-about-quantization-method-1-bit-or-sub-1-bit">Related work about quantization method (1-bit or sub 1-bit)</h2> <h3 id="bitnet-2023-3">BitNet (2023) [3]</h3> <p>This was the first study to introduce QAT to a 1-bit LLM. In that paper, the authors proposed a scalable and stable 1-bit Transformer architecture and, in particular, introduced BitLinear, a 1-bit weight layer that can replace the standard fully connected layer.</p> <div style="text-align: center;"> <img src="/assets/img/BitNet.png" alt="BitNet Architecture" style="max-width: 100%; height: 300px; object-fit: contain;"> </div> <p><br> BitNet retains the familiar Transformer stacking of self-attention and feed-forward blocks, but replaces all weight matrix multiplications with BitLinear, which uses 1-bit weights. All other operations—including residual connections and LayerNorm—remain in 8-bit precision. This design choice is justified because (1) residual paths and LayerNorm incur only negligible compute and memory cost compared to the core Transformer operations, and (2) high-precision probabilities are required for accurate sampling.</p> <ol> <li> <p><strong>Weight Binarization</strong><br> First, the weight (W) is binarized to ({+1, -1}) via a sign function.<br> To zero-center the weights, subtract the mean (\alpha), then use the scale factor (\alpha) to minimize the binarization error.</p> \[\alpha = \frac{1}{nm} \sum_{i,j} W_{ij}, \quad \widetilde{W} = \operatorname{Sign}(W - \alpha), \quad \operatorname{Sign}(x) = \begin{cases} +1, &amp; x &gt; 0,\\ -1, &amp; x \le 0. \end{cases}\] </li> <li> <p><strong>Activation AbsMax Quantization</strong><br> The activation (x) is quantized to (b) bits.</p> \[\widetilde{x} = \mathrm{Quant}(x) = \mathrm{Clip}\!\Bigl(x \times \tfrac{Q_b}{\gamma},\,-Q_b + \epsilon,\;Q_b - \epsilon\Bigr), \\ \mathrm{Clip}(z,a,b) = \max\!\bigl(a,\min(b,z)\bigr), \\ \gamma = \|x\|_\infty.\] <p>Then, subtract the minimum (\eta = \min_{i,j} x_{ij}) from all values to shift into a fixed positive range, and apply the same quantization:</p> \[\widetilde{x} = \mathrm{Quant}(x) = \mathrm{Clip}\!\Bigl((x - \eta)\times \tfrac{Q_b}{\gamma},\,-Q_b+\epsilon,\;Q_b-\epsilon\Bigr), \quad \eta = \min_{i,j} x_{ij}.\] </li> <li> <p><strong>Matrix Multiplication</strong><br> Perform matrix multiplication using the binarized weight (\widetilde{W}) and the quantized activation (\widetilde{x}):</p> \[y = \widetilde{W}\,\widetilde{x}.\] </li> <li> <p><strong>LayerNorm for Variance Preservation</strong><br> To ensure the output variance (\mathrm{Var}(y)\approx 1), apply LayerNorm before activation quantization.<br> Using Sub-LayerNorm and the aforementioned quantization methods, the BitLinear layer is defined as:</p> <p>\(\begin{aligned} y &amp;= \widetilde{W}\,\mathrm{Quant}\bigl(\mathrm{LN}(x)\bigr)\times \frac{\beta\,\gamma}{Q_b},\\ \mathrm{LN}(x) &amp;= \frac{x - \mathbb{E}[x]}{\sqrt{\mathrm{Var}(x) + \epsilon}}, \quad \beta = \frac{1}{nm}\|W\|_{1}. \end{aligned}\)</p> <h3 id="bitnet-158-2024-4">BitNet 1.58 (2024) [4]</h3> <p>Based on the BitNet Transformer architecture (replacing <code class="language-plaintext highlighter-rouge">nn.Linear</code> with <code class="language-plaintext highlighter-rouge">BitLinear</code>), BitNet b1.58 trains with <strong>1.58-bit weights</strong> and <strong>8-bit activations</strong>.</p> </li> </ol> <ul> <li>All weight parameters are fixed to <strong>({-1, 0, +1})</strong>,<br> preserving BitNet’s compression and compute advantages while<br> virtually eliminating standard matrix multiplications.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/bitnet1.58.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <ul> <li>Key benefits: <ol> <li>Dramatically reduced memory footprint.</li> <li>Explicit feature filtering via zero-valued weights improves performance.</li> <li>Perplexity and downstream task performance are on par with FP16. <br> </li> </ol> </li> </ul> <p><strong>AbsMean Quantization Function</strong></p> <ol> <li> <strong>Scaling</strong> Scale the weight matrix (W) by its average absolute value (\gamma).</li> <li> <strong>Round &amp; Clip</strong> Round and clamp each element to the nearest value in ({-1, 0, +1}).<br> \(\widetilde{W} = \mathrm{RoundClip}\!\Bigl(\frac{W}{\gamma + \epsilon}, -1, +1\Bigr),\) \(\mathrm{RoundClip}(x,a,b) = \max\bigl(a,\min(b,\mathrm{round}(x))\bigr),\) \(\gamma = \frac{1}{nm}\sum_{i,j} |W_{ij}|.\)</li> </ol> <h3 id="sub-1-bit-quantization-stbllm-2024-5">Sub 1-bit Quantization, STBLLM (2024) [5]</h3> <p>STBLLM (Structured Binary LLM) is a new compression framework that structurally binarizes large language models (LLMs) beyond conventional 1-bit precision. While LLMs offer state-of-the-art performance, their high memory and compute demands limit deployment on resource-constrained environments. STBLLM observes that flipping a portion of binary weights in 1-bit LLMs does <strong>not significantly degrade performance</strong>, suggesting room for more aggressive compression.</p> <div style="text-align: center;"> <img src="/assets/img/stbllm_1.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <p><strong>1. Structured Sparsity with N:M Pattern</strong></p> <ul> <li>STBLLM employs N:M structured pruning to encode (N) non-zero weights in every (M)-slot group, enabling <strong>extremely high compression</strong>. <br> </li> <li>This pattern is also hardware-friendly and supports <strong>accelerated inference</strong>.</li> </ul> <p><strong>2. Standardized Importance (SI) Metric</strong></p> <ul> <li>To overcome the instability of Hessian-based importance scores, STBLLM proposes a lightweight importance metric that considers both weight magnitude and input feature norm. <br> </li> <li>Importance formula:<br> \(S_{i,j} = \sigma\bigl(\mu(|W_{i,j}|)\bigr) \cdot \left\| X_{:,j} \right\|_2\) <ul> <li>( \mu ): Mean of absolute weights</li> <li>( \sigma ): Normalization function</li> <li>( X ): Input feature matrix</li> </ul> </li> </ul> <p><strong>3. Layer-wise Adaptive Binarization</strong></p> <ul> <li>Each layer’s relative importance (\alpha_i) is computed and used to apply <strong>aggressive compression for less critical layers</strong>, and <strong>preserve more precision in key layers</strong>. <br> </li> <li>Formula:<br> \(\alpha_i = \frac{\omega_i}{\omega_{\text{total}}}\) <ul> <li>( \omega_i = |W_i|_2 ): L2 norm of layer (i)</li> <li>( \omega_{\text{total}} ): Sum of norms across all layers</li> </ul> </li> </ul> <p><strong>4. Non-salient Aware Quantization</strong></p> <ul> <li>Weights are divided into three zones based on their importance: <strong>sparse</strong>, <strong>intermediate</strong>, and <strong>dense</strong>. <br> </li> <li>Different binarization strategies are applied to each zone to reduce total bit usage while maintaining task-level performance.</li> </ul> <h2 id="methodology-for-finding-the-optimal-model-size--results">Methodology for Finding the Optimal Model Size &amp; Results</h2> <h3 id="zero-shot-accuracy-perspective">Zero-shot Accuracy Perspective</h3> <p><strong>1. Extracting the 90% Threshold</strong></p> <ul> <li>For each paper and zero-shot benchmark, we determine the minimum model size required to reach 90% of the FP16 baseline accuracy.</li> </ul> <p><strong>2. Adjusting for Under-Reporting</strong></p> <ul> <li>If even the largest reported scale fails to hit 90%, we multiply that maximum scale by 1.5 to estimate the required size (e.g., if a 3B model falls short, we assume 4.5B would meet the threshold).</li> </ul> <p><strong>3. Averaging</strong></p> <ul> <li>We first average the required scales across shared benchmarks (ARC, HellaSwag, etc.), then compute separate averages for PTQ and QAT to obtain representative values.</li> </ul> <p>Based on our survey of references [4]–[8], we extracted the optimal model sizes required to approach FP16 performance.</p> <div style="text-align: center;"> <img src="/assets/img/PTQ_table.png" alt="PTQ vs QAT Table" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <div style="text-align: center;"> <img src="/assets/img/QAT_table.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br> As shown in the table below, values highlighted in red indicate cases where even the largest model reported in the paper failed to reach 90% accuracy; in those instances, we adjusted the estimate by multiplying the maximum model size by 1.5. Since the zero-shot benchmarks vary across studies, we computed the average only over benchmarks common to all papers (highlighted in orange color). <br> <br></p> <div style="text-align: center;"> <img src="/assets/img/PTQ_QAT_graph.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <p>The accompanying chart clearly demonstrates that PTQ methods require substantially larger model sizes compared to QAT approaches.</p> <h3 id="perplexity-perspective">Perplexity Perspective</h3> <p><strong>1. Establishing a PPL ≤ 10 Criterion</strong></p> <ul> <li>Because some methods exceed the baseline PPL and others omit data, we set a consistent standard of PPL ≤ 10.</li> </ul> <p><strong>2. Collecting Required Scales</strong></p> <ul> <li>We gather the model sizes reported for PTQ and QAT methods that satisfy the PPL ≤ 10 criterion.</li> </ul> <p><strong>3. Averaging</strong></p> <ul> <li>We average these scales separately for PTQ and QAT, yielding comparable insights between the two approaches.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/PPL_table.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <h3 id="holistic-evaluation">Holistic Evaluation</h3> <div style="text-align: center;"> <img src="/assets/img/ZeroPPL.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <h2 id="conclusion">Conclusion</h2> <p>In this blog post, we conducted a comprehensive review of major 1-bit and sub-1-bit quantization studies published since 2023. We found that PTQ (Post-Training Quantization) approaches typically require model sizes of 31.6–47.8 B, whereas QAT (Quantization-Aware Training) methods only need 5–6.7 B. These insights offer crucial guidelines for balancing performance targets and resource constraints when designing and deploying 1-bit LLMs</p> <hr> <h2 id="reference">Reference</h2> <ol> <li> <p><strong>PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models</strong> Zhao, J., Zhang, M., Wang, M., Shang, Y., Zhang, K., Guan, W., Wang, Y., Zhang, M. (2025) arXiv:2502.13179 <br></p> </li> <li> <p><strong>XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</strong> Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A. (2016) arXiv:1603.05279 <br></p> </li> <li> <p><strong>BitNet: Scaling 1-bit Transformers for Large Language Models</strong> Wang, H., Ma, S., Wei, F.*, et al. (2023) arXiv:2310.11453 <br></p> </li> <li> <p><strong>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</strong> Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, F., Gu, Y., Xue, F.* (2024) arXiv:2402.17764 <br></p> </li> <li> <p><strong>STBLLM: BREAKING THE 1-BIT BARRIER WITH STRUCTURED BINARY LLMS</strong> Dong, P., Li, L., Zhong, Y., Du, D., Fan, R., Chen, Y., Tang, Z., Wang, Q., Xue, F., Guo, Y., Chu, X.* (2024) arXiv:2408.01803 <br></p> </li> <li> <p><strong>PT-BitNet: Scaling up the 1-Bit Large Language Model with Post-Training Quantization</strong> Guo, Y., Hao, Z., Shao, J., Zhou, J., Liu, X., Tong, X., Zhang, Y., Chen, Y., Chen, Y., Peng, W. &amp; Ma, Z.** (2024) <br></p> </li> <li> <p><strong>BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</strong> Wang, H., Ma, S., Wei, F.* (2025) arXiv:2504.18415 <br></p> </li> <li> <p><strong>OneBit: Towards Extremely Low-bit Large Language Models</strong> Xu, Y., Han, X., Yang, Z., Wang, S., Zhu, Q., Liu, Z., Liu, W., Che, W.* (2024) arXiv:2402.11295</p> </li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'HKHKHzx/HKHKHzx.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>