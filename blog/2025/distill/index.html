<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How many parameters are needed for 1-bit quantization? | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content="In this post, we statistically analyze the parameter expansion ratio required for the 1-bit quantization model to achieve 90% of the FP16 baseline performance. By comparing and reviewing the scaling factors of the sub 1-bit, 1-bit, and 1.58-bit models collected from various literatures, we present the optimal expansion ranges for each PTQ and QAT scheme. This provides specific guidelines for practical reference in the 1-bit quantization transition."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hkhkhzx.github.io/blog/2025/distill/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How many parameters are needed for 1-bit quantization?",
            "description": "In this post, we statistically analyze the parameter expansion ratio required for the 1-bit quantization model to achieve 90% of the FP16 baseline performance. By comparing and reviewing the scaling factors of the sub 1-bit, 1-bit, and 1.58-bit models collected from various literatures, we present the optimal expansion ranges for each PTQ and QAT scheme. This provides specific guidelines for practical reference in the 1-bit quantization transition.",
            "published": "May 27, 2025",
            "authors": [
              
              {
                "author": "Hyeon-seok Shin",
                "authorURL": "https://eesl.postech.ac.kr:51276/bbs/board.php?bo_table=sub2_2&wr_id=36",
                "affiliations": [
                  {
                    "name": "POSTECH, Republic of Korea",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How many parameters are needed for 1-bit quantization?</h1> <p>In this post, we statistically analyze the parameter expansion ratio required for the 1-bit quantization model to achieve 90% of the FP16 baseline performance. By comparing and reviewing the scaling factors of the sub 1-bit, 1-bit, and 1.58-bit models collected from various literatures, we present the optimal expansion ranges for each PTQ and QAT scheme. This provides specific guidelines for practical reference in the 1-bit quantization transition.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#exploring-extreme-low-bit-quantization-trends">Exploring extreme low-bit quantization trends</a> </div> <ul> <li> <a href="#bitnet">BitNet</a> </li> <li> <a href="#bitnet-1-58">BitNet 1.58</a> </li> <li> <a href="#sub-1-bit-quantization-stbllm">Sub 1-bit Quantization, STBLLM</a> </li> </ul> <div> <a href="#methodology-for-finding-the-scaling-factor-results">Methodology for Finding the Scaling Factor &amp; Results</a> </div> <ul> <li> <a href="#scaling-factor">Scaling Factor</a> </li> <li> <a href="#results">Results</a> </li> <li> <a href="#summary-of-key-results">Summary of key results</a> </li> </ul> <div> <a href="#contribution-conclusion">Contribution &amp; Conclusion</a> </div> <div> <a href="#limitation">Limitation</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Since the introduction of the Transformer architecture, large language models have leveraged tens to hundreds of billions of parameters to achieve remarkable language generation capabilities. However, as the model grows in size, training the model requires excessive time, resources, and energy, making it challenging for real-world deployment and low-power edge utilization.</p> <div style="text-align: center;"> <img src="/assets/img/modelsizeup.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <p>To mitigate these challenges, 8-bit and 4-bit quantization techniques have been adopted to reduce memory usage with minimal performance loss compared to FP16, but when bit-width is reduced below 2 bits, both perplexity and zero-shot accuracy suffer drastic degradation <a href="https://arxiv.org/abs/2502.13179" style="color:blue;" rel="external nofollow noopener" target="_blank">[1]</a>. Extreme low bit quantization offers the potential to reduce Memory footprint and energy consumption <a href="https://arxiv.org/abs/2310.11453" style="color:blue;" rel="external nofollow noopener" target="_blank">[2]</a>, but to offset the information loss from lower bit-widths, the model must be scaled up significantly. However, if the model size is unnecessarily increased, not only performance improvement becomes insignificant <a href="https://arxiv.org/pdf/2001.08361" style="color:blue;" rel="external nofollow noopener" target="_blank">[3]</a>, but additional computational resources and energy consumption increase and practical distribution becomes difficult. To that end, quantifying the minimum model size will be essential in terms of model distribution and operational cost-effectiveness. <strong>So identifying the “optimal model size” that maintains FP16-level performance while still achieving efficient resource savings is essential.</strong></p> <p>In particular, PTQ (Post-Training Quantization) and QAT (Quantization-Aware Training) require very different model scales, yet to date no study has quantitatively defined the optimal sizes for these two approaches. In this work, we conduct a thorough <strong>survey of the leading 1-bit training methods</strong> published since 2024 and <strong>estimated the optimal model size for PTQ and QAT</strong> by defining the scaling factor, providing concrete guidelines for balancing target performance and resource constraints when designing and deploying 1-bit LLMs.</p> <h2 id="exploring-extreme-low-bit-quantization-trends">Exploring extreme low-bit quantization trends</h2> <h3 id="bitnet-2023-2">BitNet (2023) <a href="https://arxiv.org/abs/2310.11453" rel="external nofollow noopener" target="_blank">[2]</a> </h3> <p>This was the first study to introduce QAT to a 1-bit LLM. In that paper, the authors proposed a scalable and stable 1-bit Transformer architecture and, in particular, introduced BitLinear, a 1-bit weight layer that can replace the standard fully connected layer.</p> <div style="text-align: center;"> <img src="/assets/img/BitNet.png" alt="BitNet Architecture" style="max-width: 100%; height: 300px; object-fit: contain;"> </div> <p><br> BitNet retains the familiar Transformer stacking of self-attention and feed-forward blocks, but replaces all weight matrix multiplications with BitLinear, which uses 1-bit weights. All other operations—including residual connections and LayerNorm—remain in 8-bit precision. This design choice is justified because (1) residual paths and LayerNorm incur only negligible compute and memory cost compared to the core Transformer operations, and (2) high-precision probabilities are required for accurate sampling.</p> <details> <summary style="color: green;">View BitLinear's quantization &amp; computational procedures</summary> <ol> <li> <p><strong>Weight Binarization</strong><br> First, the weight $W$ is binarized to ({+1, -1}) via a sign function.<br> To zero-center the weights, subtract the mean $\alpha$, then use the scale factor $\alpha$ to minimize the binarization error.</p> \[\alpha = \frac{1}{nm} \sum_{i,j} W_{ij}, \quad \widetilde{W} = \operatorname{Sign}(W - \alpha), \quad \operatorname{Sign}(x) = \begin{cases} +1, &amp; x &gt; 0,\\ -1, &amp; x \le 0. \end{cases}\] </li> <li> <p><strong>Activation AbsMax Quantization</strong><br> The activation $x$ is quantized to $b$ bits.</p> <p>\(\widetilde{x} = \mathrm{Quant}(x) = \mathrm{Clip}\!\Bigl(x \times \tfrac{Q_b}{\gamma},\,-Q_b + \epsilon,\;Q_b - \epsilon\Bigr),\) \(\mathrm{Clip}(z,a,b) = \max\!\bigl(a,\min(b,z)\bigr),\) \(\gamma = \|x\|_\infty.\)</p> <p>Then, subtract the minimum $\eta$ = $\min_{i,j} x_{ij}$ from all values to shift into a fixed positive range, and apply the same quantization:</p> \[\widetilde{x} = \mathrm{Quant}(x) = \mathrm{Clip}\!\Bigl((x - \eta)\times \tfrac{Q_b}{\gamma},\,-Q_b+\epsilon,\;Q_b-\epsilon\Bigr), \quad \eta = \min_{i,j} x_{ij}.\] </li> <li> <p><strong>Matrix Multiplication</strong><br> Perform matrix multiplication using the binarized weight $\widetilde{W}$ and the quantized activation $\widetilde{x}$:</p> \[y = \widetilde{W}\,\widetilde{x}.\] </li> <li> <p><strong>LayerNorm for Variance Preservation</strong><br> To ensure the output variance $\mathrm{Var}(y)\approx 1$, apply LayerNorm before activation quantization.<br> Using Sub-LayerNorm and the aforementioned quantization methods, the BitLinear layer is defined as:</p> \[\begin{aligned} y &amp;= \widetilde{W}\,\mathrm{Quant}\bigl(\mathrm{LN}(x)\bigr)\times \frac{\beta\,\gamma}{Q_b},\\ \mathrm{LN}(x) &amp;= \frac{x - \mathbb{E}[x]}{\sqrt{\mathrm{Var}(x) + \epsilon}}, \quad \beta = \frac{1}{nm}\|W\|_{1}. \end{aligned}\] </li> </ol> </details> <h3 id="bitnet-158-2024-4">BitNet 1.58 (2024) <a href="https://arxiv.org/abs/2402.17764" rel="external nofollow noopener" target="_blank">[4]</a> </h3> <p>Although it is based on the BitNet transformer architecture, we introduced a variant of 1-bit LLM with all individual parameters of <strong>{-1, 0, 1}</strong>. Despite its comparable performance to Full-Precision Transformer LLM, it dramatically reduced computation/memory cost. In-depth, a new scaling scheme was presented, and a computational Pradigm for new hardware for 1-bit LLM targets was presented.</p> <details> <summary style="color: green;">Brief description of BitNet 1.58b and AbsMean quantization techniques</summary> <ul> <li>All weight parameters are fixed to <strong>({-1, 0, +1})</strong>, preserving BitNet’s compression and compute advantages while<br> virtually eliminating standard matrix multiplications.</li> </ul> <div style="text-align: center;"> <img src="/assets/img/bitnet1.58.png" alt="BitNet Architecture" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <ul> <li>Key benefits: <ol> <li>Dramatically reduced memory footprint.</li> <li>Explicit feature filtering via zero-valued weights improves performance.</li> <li>Perplexity and downstream task performance are on par with FP16. <br> </li> </ol> </li> </ul> <p><strong>AbsMean Quantization Function</strong></p> <ol> <li> <strong>Scaling</strong> Scale the weight matrix $W$ by its average absolute value $\gamma$.</li> <li> <strong>Round &amp; Clip</strong> Round and clamp each element to the nearest value in ({-1, 0, +1}).<br> \(\widetilde{W} = \mathrm{RoundClip}\!\Bigl(\frac{W}{\gamma + \epsilon}, -1, +1\Bigr),\) \(\mathrm{RoundClip}(x,a,b) = \max\bigl(a,\min(b,\mathrm{round}(x))\bigr),\) \(\gamma = \frac{1}{nm}\sum_{i,j} |W_{ij}|.\)</li> </ol> </details> <h3 id="sub-1-bit-quantization-stbllm-2024-5">Sub 1-bit Quantization, STBLLM (2024) <a href="https://arxiv.org/abs/2408.01803" rel="external nofollow noopener" target="_blank">[5]</a> </h3> <div style="text-align: center;"> <img src="/assets/img/stbllm_1.png" alt="BitNet Architecture" style="max-width: 70%; height: auto; object-fit: contain;"> </div> <p><br></p> <p>STBLLM (Structured Binary LLM) is a new compression framework that structurally binarizes large language models (LLMs) beyond conventional 1-bit precision. While LLMs offer state-of-the-art performance, their high memory and compute demands limit deployment on resource-constrained environments. STBLLM observes that sparsifying a portion of binary weights in 1-bit LLMs does <strong>not significantly degrade performance</strong>, suggesting room for more aggressive compression.</p> <details> <summary style="color: green;">Summary of STBLLM Four Key Techniques</summary> <p><strong>1. Structured Sparsity with N:M Pattern</strong></p> <ul> <li>STBLLM employs N:M structured pruning to encode (N) non-zero weights in every (M)-slot group, enabling <strong>extremely high compression</strong>. <br> </li> <li>This pattern is also hardware-friendly and supports <strong>accelerated inference</strong>.</li> </ul> <p><strong>2. Standardized Importance (SI) Metric</strong></p> <ul> <li>To overcome the instability of Hessian-based importance scores, STBLLM proposes a lightweight importance metric that considers both weight magnitude and input feature norm. <br> </li> <li>Importance formula:<br> \(S_{i,j} = \sigma\bigl(\mu(|W_{i,j}|)\bigr) \cdot \left\| X_{:,j} \right\|_2\) <ul> <li>$\mu$: Mean of absolute weights</li> <li>$\sigma$: Normalization function</li> <li>$X$: Input feature matrix</li> </ul> </li> </ul> <p><strong>3. Layer-wise Adaptive Binarization</strong></p> <ul> <li>Each layer’s relative importance $\alpha_i$ is computed and used to apply <strong>aggressive compression for less critical layers</strong>, and <strong>preserve more precision in key layers</strong>. <br> </li> <li>Formula:<br> \(\alpha_i = \frac{\omega_i}{\omega_{\text{total}}}\) <ul> <li>$\omega_i$ = $|W_i|_2$: L2 norm of layer (i)</li> <li>$\omega_{\text{total}}$: Sum of norms across all layers</li> </ul> </li> </ul> <p><strong>4. Non-salient Aware Quantization</strong></p> <ul> <li>Weights are divided into three zones based on their importance: <strong>sparse</strong>, <strong>intermediate</strong>, and <strong>dense</strong>. <br> </li> <li>Different binarization strategies are applied to each zone to reduce total bit usage while maintaining task-level performance.</li> </ul> </details> <h2 id="methodology-for-finding-the-scaling-factor--results">Methodology for Finding the Scaling Factor &amp; Results</h2> <h3 id="scaling-factor">Scaling Factor</h3> <p>The model size required to perform more than 90% of the FP16 baseline zero-shot accuracy is expressed as a scaling factor.</p> <p>\(Scaling\ Factor = \frac{Model\ size\ for\ Low\ bit\ training\ method}{Model\ size\ for\ FP16\ baseline}\) <br> Simply, it represents the ratio of the model size trained with FP16 to the model size when having similar performance.</p> <p><strong>Example</strong>: 1-bit quantization model requires 1.8× the size of the original model to perform 90% → Scaling Factor = 1.8</p> <h3 id="results">Results</h3> <p>Based on our survey of references <a href="https://arxiv.org/abs/2402.17764" style="color:blue;" rel="external nofollow noopener" target="_blank">[4]</a>–<a href="https://arxiv.org/abs/2402.11295" style="color:blue;" rel="external nofollow noopener" target="_blank">[8]</a>, we extracted the scaling factor required to approach FP16 performance. For PTQ and QAT, the scaling factors according to the bit format are shown in the table below. When the model size tested in the reference was less than 90% of the performance of the zero-shot accuracy, it was marked as <span style="color: red;">Inconclusive (Red).</span></p> <div style="text-align: center;"> <img src="/assets/img/Scaling_Factor.png" alt="PTQ vs QAT Table" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <ul> <li> <p>Since the PT-BitNet paper itself has no direct comparison results with the FP16 baseline, we compared it against the Full-Precision baseline performance presented in the <a href="https://arxiv.org/abs/2408.01803" style="color:blue;" rel="external nofollow noopener" target="_blank">[5]</a>.</p> </li> <li> <p>BitNet_v2 series took and used the values compared to the FP16 baseline provided by the <a href="https://arxiv.org/abs/2402.17764" style="color:blue;" rel="external nofollow noopener" target="_blank">[4]</a> as it was.</p> </li> </ul> <p>In the table, the light green area is a common benchmark evaluated in both PTQ and QAT, limited to W1.58A8 format. Through the following graph, it is possible to determine which method of PTQ and QAT may more efficiently minimize the parameter expansion through fair conditions (same bit width, same format).</p> <div style="text-align: center;"> <img src="/assets/img/PTQ_QAT_Format.png" alt="PTQ vs QAT Table" style="max-width: 100%; height: auto; object-fit: contain;"> </div> <p><br></p> <h3 id="summary-of-key-results">Summary of key results</h3> <ul> <li> <p><strong>Sub 1-bit training</strong> <br>The Sub 1-bit method fell short of the baseline on almost all benchmarks.(especially A4)</p> </li> <li> <p><strong>1-bit training</strong> <br>Lowering the weight to 1-bit in the QAT scheme did not achieve a reference point on most benchmarks, even if the activation bit was high.</p> </li> <li> <p><strong>1.58-bit training</strong> <br>The PTQ method requires a model size that is two to five times higher than the QAT.</p> </li> </ul> <p>In the case of the PTQ method, it can be seen that the reference point is below the reference point in most benchmarks. It is believed that QAT requires a smaller model than PTQ because PTQ only quantizes the already learned model as it is, while QAT reflects the quantization process during learning to help parameters correct the ‘quantization error’ when learning. In other words, the efficiency of PTQ drops sharply when the bit-width is extended low<a href="https://arxiv.org/abs/2402.11295" style="color:blue;" rel="external nofollow noopener" target="_blank">[8]</a>, suggesting the insight that QAT is essential for extreme low bit training.</p> <h2 id="contribution--conclusion">Contribution &amp; Conclusion</h2> <p>In this blog post, we conducted a comprehensive review of major 1-bit and sub 1-bit quantization studies published since 2023. We found that PTQ approaches typically require model sizes than QAT methods. In addition, we propose the possibility that quantizing the weight to less than 1.58 bits in the QAT scheme can dramatically increase the required model size. In this study, <strong>we quantitatively analyzed how much model parameters should be expanded for 1-bit quantization</strong>. This provides practical guidelines on how much model size should be increased when converting individual neural networks to 1-bit.</p> <h2 id="limitation">Limitation</h2> <p>Since it has set a subjective standard of 90%, it may lack persuasive power in objective terms.</p> <hr> <p><small></small></p> <h2 id="reference">Reference</h2> <p><a href="https://arxiv.org/abs/2502.13179" rel="external nofollow noopener" target="_blank">1. <strong>PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models</strong></a></p> <p>Zhao, J., Zhang, M., Wang, M., Shang, Y., Zhang, K., Guan, W., Wang, Y., Zhang, M. (2025) arXiv:2502.13179 <br></p> <p><a href="https://arxiv.org/abs/2310.11453" rel="external nofollow noopener" target="_blank">2. <strong>BitNet: Scaling 1-bit Transformers for Large Language Models</strong></a></p> <p>Wang, H., Ma, S., Wei, F.*, et al. (2023) arXiv:2310.11453 <br></p> <p><a href="https://arxiv.org/pdf/2001.08361" rel="external nofollow noopener" target="_blank">3. <strong>Scaling Laws for Neural Language Models</strong></a></p> <p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D. (2020) arXiv:2001.08361 <br></p> <p><a href="https://arxiv.org/abs/2402.17764" rel="external nofollow noopener" target="_blank">4. <strong>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</strong> </a></p> <p>Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, F., Gu, Y., Xue, F.* (2024) arXiv:2402.17764 <br></p> <p><a href="https://arxiv.org/abs/2408.01803" rel="external nofollow noopener" target="_blank">5. <strong>STBLLM: BREAKING THE 1-BIT BARRIER WITH STRUCTURED BINARY LLMS</strong> </a></p> <p>Dong, P., Li, L., Zhong, Y., Du, D., Fan, R., Chen, Y., Tang, Z., Wang, Q., Xue, F., Guo, Y., Chu, X.* (2024) arXiv:2408.01803 <br></p> <p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4987078" rel="external nofollow noopener" target="_blank">6. <strong>PT-BitNet: Scaling up the 1-Bit Large Language Model with Post-Training Quantization</strong> </a></p> <p>Guo, Y., Hao, Z., Shao, J., Zhou, J., Liu, X., Tong, X., Zhang, Y., Chen, Y., Chen, Y., Peng, W. &amp; Ma, Z.** (2024) <br></p> <p><a href="https://arxiv.org/abs/2504.18415" rel="external nofollow noopener" target="_blank">7. <strong>BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs</strong> </a></p> <p>Wang, H., Ma, S., Wei, F.* (2025) arXiv:2504.18415 <br></p> <p><a href="https://arxiv.org/abs/2402.11295" rel="external nofollow noopener" target="_blank">8. <strong>OneBit: Towards Extremely Low-bit Large Language Models</strong> </a></p> <p>Xu, Y., Han, X., Yang, Z., Wang, S., Zhu, Q., Liu, Z., Liu, W., Che, W.* (2024) arXiv:2402.11295</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2018-12-22-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'HKHKHzx/HKHKHzx.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>